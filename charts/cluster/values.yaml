---
exports:
  data:
    capi:
      providers:
        core:
          name: cluster-api
        # https://cluster-api.sigs.k8s.io/tasks/bootstrap/kubeadm-bootstrap/#kubeadmconfig-objects
        # https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta3/
        controlPlane:
          name: kubeadm
        bootstrap:
          name: kubeadm
        infrastructure:
          name: hetzner
        # # https://github.com/siderolabs/cluster-api-control-plane-provider-talos?tab=readme-ov-file#creating-your-own-templates
        # # Creates CP nodes and bootstraps them with Talos
        # controlPlane:
        #   name: talos
        # # https://www.talos.dev/v1.8/reference/configuration/v1alpha1/config/
        # # https://github.com/siderolabs/cluster-api-bootstrap-provider-talos?tab=readme-ov-file#usage
        # # CABPT is not used directly, but rather via CACPPT (TalosControlPlane) for control plane nodes or
        # # via MachineDeployment (MachinePool) for worker nodes.
        # # https://doc.crds.dev/github.com/kubernetes-sigs/cluster-api/cluster.x-k8s.io/MachineDeployment/v1beta1@v1.8.5
        # # MachineDeployment creates worker nodes via TalosConfigTemplate
        # bootstrap:
        #   name: talos

    cluster:
      # Default is the release name.
      name: ""

    networking:
      apiPort: 6443
      # networkIpv4Cidr: ""
      # subnetIpv4Cidr: ""
      # servicesIpv4Cidr: ""
      # podsIpv4Cidr: ""

    machines:
      cp:
        osVersion: "2404"
        k8sVersion: "v1.31.4"
        buildTimestamp: ""
        imageName: >-
          {{- $machines := (include "machines" $) | fromYaml -}}
          cluster-api-ubuntu-{{ $machines.cp.osVersion }}-{{ $machines.cp.k8sVersion }}-{{ $machines.cp.buildTimestamp }}
        replicas: 3
        # https://cluster-api.sigs.k8s.io/tasks/automated-machine-management/healthchecking
        # https://doc.crds.dev/github.com/kubernetes-sigs/cluster-api/cluster.x-k8s.io/MachineHealthCheck/v1beta1@v1.8.5
        healthCheck:
          enabled: true
          # maxUnhealthy prevents further remediation if the cluster is already partially unhealthy.
          # If the number of unhealthy Machines exceeds the limit set by maxUnhealthy, remediation will **not** be performed.
          # For example if we have 3 CPs and maxUnhealthy is set to 1 (40%), and if 2 CPs are unhealthy, no remediation will be performed.
          #
          # To ensure that MachineHealthChecks only remediate Machines when the cluster is healthy, short-circuiting is implemented.
          # The default value for maxUnhealthy is 100%. This means the short circuiting mechanism is disabled by default and
          # Machines will be remediated no matter the state of the cluster.
          # Remediation short circuiting : https://cluster-api.sigs.k8s.io/tasks/automated-machine-management/healthchecking#remediation-short-circuiting
          #
          # Deprecated: This field is deprecated and is going to be removed in the next apiVersion.
          # Please see https://github.com/kubernetes-sigs/cluster-api/issues/10722 for more details.
          maxUnhealthy: 100%
          # nodeStartupTimeout determines how long a MachineHealthCheck should wait for a Node to join the cluster, before considering a Machine unhealthy.
          nodeStartupTimeout: 15m0s
          # Conditions to check on Nodes for matched Machines, if any condition is matched for the duration of its timeout,
          # the Machine is considered unhealthy.
          # `k get -n mgt-cluster Machine capi-talos-XYZ -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'`
          unhealthyConditions:
          - type: Ready
            status: Unknown
            timeout: 5m0s
          - type: Ready
            status: "False"
            timeout: 5m0s
      worker:
        osVersion: "2404"
        k8sVersion: "v1.31.4"
        buildTimestamp: ""
        imageName: >-
          {{- $machines := (include "machines" $) | fromYaml -}}
          cluster-api-ubuntu-{{ $machines.worker.osVersion }}-{{ $machines.worker.k8sVersion }}-{{ $machines.worker.buildTimestamp }}
        replicas: 3
        autoscaler:
          enabled: false
          minSize: ""
          maxSize: ""
        healthCheck:
          enabled: true
          maxUnhealthy: 100%
          nodeStartupTimeout: 10m0s
          unhealthyConditions:
          - type: Ready
            status: Unknown
            timeout: 3m0s
          - type: Ready
            status: "False"
            timeout: 3m0s

    hCloud:
      # https://syself.com/docs/caph/topics/managing-ssh-keys
      sshKeys:
      # - "name"
      token:
        # Use an existing secret or create a new one.
        existingSecret: false
        # Token value to use when creating new secret.
        value: ""
        # Name of the secret to create or use (existing secret).
        secretName: "hcloud"
        secretKey: "token"
        # For when creating new secret and for CAPI mgt clusters only.
        # Adds `clusterctl.cluster.x-k8s.io/move` label or not.
        move: false
      networking:
        # apiPort: 6443
        # Enable private hCloud network for the cluster.
        hcloudNetworkEnabled: false
        # https://docs.hetzner.com/cloud/general/locations/
        # https://docs.hetzner.com/cloud/networks/faq/#can-networks-span-multiple-locations
        region: "fsn1"
        # Required only private hCloud network is enabled (hcloudNetworkEnabled).
        # networkZone: eu-central
        # Considerations on the IP Ranges. Required only private hCloud network is enabled.
        # https://github.com/hetznercloud/hcloud-cloud-controller-manager/blob/v1.20.0/docs/deploy_with_networks.md#considerations-on-the-ip-ranges
        # https://docs.hetzner.com/cloud/networks/faq/#which-ip-addresses-can-i-use
        # https://docs.hetzner.com/cloud/networks/faq/#are-any-ip-addresses-reserved
        # https://docs.hetzner.com/cloud/networks/faq/#are-there-any-limits-on-how-networks-can-be-used
        # networkIpv4Cidr: ""
        # Required only private hCloud network is enabled (hcloudNetworkEnabled).
        # https://docs.hetzner.com/cloud/networks/faq/#what-are-subnets
        # subnetIpv4Cidr: ""
        servicesIpv4Cidr: ""
        podsIpv4Cidr: ""
      lb:
        existing:
          # Defaults to the release name.
          name: ""
          ip: ""
          # extraServices:
          # # Talos API
          # # https://www.talos.dev/v1.8/introduction/getting-started/#accessing-the-talos-api
          # # https://www.talos.dev/v1.8/introduction/prodnotes/#load-balancing-the-talos-api
          # - destinationPort: 50000
          #   listenPort: 50000
          #   protocol: tcp
        new:
          # extraServices:
          # # Talos API
          # # https://www.talos.dev/v1.8/introduction/getting-started/#accessing-the-talos-api
          # # https://www.talos.dev/v1.8/introduction/prodnotes/#load-balancing-the-talos-api
          # - destinationPort: 50000
          #   listenPort: 50000
          #   protocol: tcp
          # algorithm: round_robin
          # port: "6443"
          # region: ""
          # type: lb11
      machines:
        # List of placement groups that should be defined in Hetzner API
        # https://docs.hetzner.com/cloud/placement-groups/overview
        # https://docs.hetzner.com/cloud/placement-groups/faq#are-placement-groups-location-bound
        placementGroups:
        - name: cp
          type: spread
        - name: worker
          type: spread
        cp:
          type: "cpx31"
          placementGroupName: cp
          # https://syself.com/docs/caph/reference/hcloud-remediation-template
          remediation:
            # Enable HCloudRemediation for control plane nodes if needed.
            # Default remediation (MachineHealthChecks) skips CP nodes.
            # > Please note that MachineHealthChecks currently only support Machines that are owned by a MachineSet or a KubeadmControlPlane.
            # https://cluster-api.sigs.k8s.io/tasks/automated-machine-management/healthchecking#limitations-and-caveats-of-a-machinehealthcheck
            enabled: false
            # RetryLimit sets the maximum number of remediation retries. Zero retries if not set.
            retryLimit: 3
            # Timeout sets the timeout between remediation retries. It should be of the form "10m", or "40s".
            timeout: 3m0s
            # By default, the action of remediating a Machine should trigger a new Machine to be created to replace the failed one,
            # but providers are allowed to plug in more sophisticated external remediation solutions.
            # ref: https://cluster-api.sigs.k8s.io/tasks/automated-machine-management/healthchecking#what-is-a-machinehealthcheck
            # Type represents the type of the remediation strategy. At the moment, only "Reboot" is supported.
            type: Reboot
        worker:
          type: "cpx31"
          placementGroupName: worker
          remediation:
            # Disable HCloudRemediationTemplate and rely on default remediation
            # which should trigger a new Machine to be created to replace the failed one.
            enabled: false
            retryLimit: 3
            timeout: 3m0s
            type: Reboot

    # To write custom config into /etc/kubernetes/ in CP nodes.
    etcKubernetes:
      gitRepoUrl: ""  # https://github.com/unbyte-de/etc-kubernetes.git

    # https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
    admissionPlugins:
      # In Kubernetes 1.31, the default ones are:
      #   CertificateApproval, CertificateSigning, CertificateSubjectRestriction, DefaultIngressClass, DefaultStorageClass,
      #   DefaultTolerationSeconds, LimitRanger, MutatingAdmissionWebhook, NamespaceLifecycle, PersistentVolumeClaimResize,
      #   PodSecurity, Priority, ResourceQuota, RuntimeClass, ServiceAccount, StorageObjectInUseProtection,
      #   TaintNodesByCondition, ValidatingAdmissionPolicy, ValidatingAdmissionWebhook
      # Here is the list of plugins we extend:
      enable:
      - AlwaysPullImages
      - EventRateLimit
      - NodeRestriction
      - PodNodeSelector
      # https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#eventratelimit
      # https://kubernetes.io/docs/reference/config-api/apiserver-eventratelimit.v1alpha1/
      eventRateLimit:
        enabled: true
      # https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecurity
      # https://kubernetes.io/docs/concepts/security/pod-security-admission/
      # https://kubernetes.io/docs/concepts/security/pod-security-standards/
      # https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller
      # https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/
      podSecurity:
        # kyverno is more flexible in terms of policy definiton and exceptions.
        enabled: true
        defaultAudit: restricted
        defaultWarn: restricted
        defaultEnforce: restricted
        exemptions:
          usernames: []
          runtimeClasses: []
          namespaces:
          - kube-system

    kubeadm:
      # If kubeProxy is disabled, cilium config requires a configmap "k8s-service-host" in namespace kube-system
      # with key "host" holding the IP of the kube-apiserver LB.
      disableKubeProxy: false
      setKubeletNodeIp: false
      etcd:
        encryption:
          # Requires a secret with the EncryptionConfiguration.
          # The secret must be in the same namespace as cluster definition in CAPI management cluster.
          # https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
          enabled: false
          secretName: etcd-encryption-configuration
          secretKey: encryption-provider.yaml
      kubeApiServer:
        # TODO: disableAnonymousAuth is not possible to enable at the moment.
        # When anonymous-auth is disabled, kubeadm can't join,
        # because it initially needs access to cluster-info configmap (in kube-public ns).
        # See also the similar issue in kube-spray:
        # https://github.com/kubernetes-sigs/kubespray/pull/11016#issuecomment-2004985001
        # https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/hardening.md
        # TODO: disablePublicAccessClusterInfoConfigMap is not possible to enable at the moment.
        # This doesn't work because I couldn't figure out how to generate tlsBootstrapToken or kubeconfig user data.
        # Here is the error returned by `kubeadm join --config /run/kubeadm/kubeadm-join-config.yaml`:
        # > error execution phase preflight: couldn't find authentication credentials for the TLS boostrap process.
        #   Please use Token discovery, a discovery file with embedded authentication credentials or
        #   a discovery file without authentication credentials but with the TLSBootstrapToken flag
        # disableAnonymousAuth: false
        # # But at least we can remove public access to the cluster-info configmap.
        # # Ref: https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/#turning-off-public-access-to-the-cluster-info-configmap
        # #      https://github.com/kubernetes-sigs/cluster-api/pull/10799
        # # Similar implementation in kube-spray: https://github.com/kubernetes-sigs/kubespray/pull/11016
        # disablePublicAccessClusterInfoConfigMap: false
        enableAuditLogs: true
        oidc:
          enabled: false
          issuerUrl: ""
          clientId: ""
          caFile: /etc/ssl/certs/acme-ca.crt
          useExistingSecretForCa: true
          usernamePrefix: oidc#
          usernameClaim: email
          groupsPrefix: oidc#
          groupsClaim: groups
        # There two flags control how long pods without explicit tolerations will “stick with” a node that goes
        # NotReady or Unreachable before being evicted and rescheduled. Default is 300s (5 min).
        # https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/.
        defaultNotReadyTolerationSeconds: "300"
        defaultUnreachableTolerationSeconds: "300"
      postKubeadm:
      #   extraFiles:
        extraCommandsCp: ""
        extraCommandsWorker: ""

    talos:
      etcd:
        version: "v3.5.16"
        electionTimeout: 5000
      machine:
        # https://www.talos.dev/v1.8/talos-guides/configuration/pull-through-cache/#using-harbor-as-a-caching-registry
        registries:
          mirrors:
            # registry.k8s.io:
            #   endpoints:
            #   -
            #   overridePath: true
            # docker.io:
            #   endpoints:
            #   -
            #   overridePath: true
            # ghcr.io:
            #   endpoints:
            #   -
            #   overridePath: true
            # gcr.io:
            #   endpoints:
            #   -
            #   overridePath: true
            # quay.io:
            #   endpoints:
            #   -
            #   overridePath: true

    # Here we define initial resources which are applied by CAPI (ClusterResourceset) just after cluster creation.
    # Install Cilium, HCCM and TCCM in workload cluster using ClusterResourceSet.
    # This creates a Job that renders helm templates for the resources and then creates a secret for each resource,
    # which are then applied in workload cluster by the ClusterResourceSet controller.
    # Note: Set `install.enabled` to `false` after first install, so secrets are cleaned up.
    # https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-resource-set#update-from-applyonce-to-reconcile
    # https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20200220-cluster-resource-set.md
    install:
      enabled: true
      # This is only in effect if `enabled` is false. Then all resources related for this initial install process will be deleted.
      deleteAll: false
      # > If ClusterResourceSet resources will be managed by an operator after they are applied by ClusterResourceSet controller,
      # "ApplyOnce" mode must be used so that reconciliation on those resources can be delegated to the operator.
      # ref: https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20200220-cluster-resource-set.md#clusterresourceset-object-definition
      strategy: ApplyOnce
      k8sVersion: "1.31.4"
      secretLabel:
        key: name
        value: cluster-resource-set-install-once
      # yamllint disable rule:line-length
      # TODO renovate
      jobImage: harbor.devops1.pbm.sh/proxy-registry.gitlab.com/unbytede/unbyte-orbit/container-images/helm-kubectl:main@sha256:160a72c617212460d1355bd4518019eb222b81296566c417bf91ef836e87aa3d
      # yamllint enable rule:line-length
      # extraRoles:
      resources:
        10-cilium:
          enabled: true
          repoUrl: https://helm.cilium.io/
          # repoCreds:
          #   username: ""
          #   password: ""
          chart: cilium
          releaseName: cilium
          version: 1.18.0
          namespace: kube-system
          createNamespace: false
          secretsCount: 1
          values: |-
            rollOutCiliumPods: true
            priorityClassName: "system-node-critical"
            hubble:
              enabled: false
            operator:
              rollOutPods: true
              priorityClassName: "system-node-critical"
              replicas: 2  # Default is 2
              # Let operator to be scheduled on empty nodes
              # https://github.com/cilium/cilium/pull/40137
              tolerations:
              - key: "node-role.kubernetes.io/control-plane"
                operator: Exists
              - key: "node-role.kubernetes.io/master" #deprecated
                operator: Exists
              - key: "node.kubernetes.io/not-ready"
                operator: Exists
              - key: "node.cilium.io/agent-not-ready"
                operator: Exists
              # Added by us.
              - key: "node.cloudprovider.kubernetes.io/uninitialized"
                operator: Equal
                value: "true"
                effect: NoSchedule
              - key: "node.cluster.x-k8s.io/uninitialized"
                operator: Exists
                effect: NoSchedule
            {{- if .Values.kubeadm.disableKubeProxy }}
            k8sServiceHostRef:
              name: k8s-service-host
              key: host
            k8sServicePort: "6443"
            {{- end }}
            devices: "eth+"
            ipam:
              mode: "kubernetes"
            {{- if .Values.kubeadm.disableKubeProxy }}
            socketLB:
              enabled: true
              hostNamespaceOnly: true
            kubeProxyReplacement: "true"
            {{- else }}
            kubeProxyReplacement: false
            {{- end }}
            loadBalancer:
              algorithm: maglev
            {{- $networking := (include "networking" $) | fromYaml }}
            {{- if $networking.hcloudNetworkEnabled  }}
            routingMode: "native"
            ipv4NativeRoutingCIDR: {{ $networking.podsIpv4Cidr | quote }}
            {{- else }}
            routingMode: tunnel
            {{- end }}
            bandwidthManager:
              enabled: true
            bpf:
              masquerade: true
            encryption:
              enabled: true
              type: wireguard
              nodeEncryption: true
            clustermesh:
              apiserver:
                tls:
                  auto:
                    method: cronJob
            hostFirewall:
              enabled: true
            # Audit mode will be disabled during argocd sync after policies are applied.
            policyAuditMode: true
            policyEnforcementMode: always
        20-hccm:
          enabled: true
          repoUrl: https://charts.hetzner.cloud
          chart: hcloud-cloud-controller-manager
          releaseName: hccm
          version: 1.26.0
          namespace: kube-system
          createNamespace: false
          secretsCount: 1
          values: |-
            env:
              HCLOUD_TOKEN:
                valueFrom:
                  secretKeyRef:
                    name: hcloud
                    key: token
            networking:
              {{- $networking := (include "networking" $) | fromYaml }}
              {{- if $networking.hcloudNetworkEnabled  }}
              enabled: true
              clusterCIDR: {{ $networking.podsIpv4Cidr | quote }}
              network:
                valueFrom:
                  secretKeyRef:
                    name: hcloud
                    key: network
              {{- else }}
              enabled: false
              {{- end }}
        30-tccm:
          enabled: false
          repoUrl: oci://ghcr.io/siderolabs/charts/talos-cloud-controller-manager
          releaseName: tccm
          version: 0.4.4
          namespace: kube-system
          createNamespace: false
          secretsCount: 1
          values: |-
            daemonSet:
              enabled: true
